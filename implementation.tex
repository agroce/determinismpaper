\section{Implementation}


\subsection{Sources of Nondeterminism in Practice}

In order to implement a practical nondeterminism-detection tool, it is
important to consider the sources of nondeterminism.  Most of these
sources can, in principle, be detected simply by re-running a test
twice and comparing results, either at all visible values or at the
final state.  For example, if some library call uses a random number
generator (perhaps in a stochastic algorithm), the value should be
different on a second run.  Timing dependencies may change on a second
execution, especially if a delay is added between steps of the test (a
more principled version of the unfortunate tradition of testing
concurrent code by adding ad-hoc {\tt sleep} statements).   Most
nondeterministic behavior in tests is probably attributable to a such
effects that can vary with a repeated execution, in the same
(approximate) enviroment, of the same test.  The two broad classes of
nondeterminism that can usually be detected (with some probability) by
simply executing a test twice in succession are:

\begin{itemize}
\item {\bf External Environment:} This class of nondeterminism arises
  when a test's behavior depends on factors outside the program under
  test, which are subject to uncontrolled variance.  Calling {\tt
    random()} in Python is an obvious example; the call is almost
  guaranteed to return a different value each time it is called in a
  test, unless we explicitly re-seed the random number generator at
  the beginning of each test.  Similarly, calls to {\tt time}
  functions  will usually return different values on different runs,
  and even if only elapsed time, rather than absolute, is used to
  influence behavior, subtle variances in timing are to be expected
  due to system load, I/O, and other external factors.  Finally, in
  distributed system testing, the environment is often remote servers
  (or clients) that may be unavailable, slower than expected, or
  subject to faulty behavior for reasons not under the control of the test.
\item {\bf Concurrency:} Perhaps the second most common cause for
  nondeterminism in tests is when an algorithm is implemented using
  concurrency, in the (incorrect) beleif that the result of the
  algorithm remains deterministic.  E.g., consider a multi-threaded
  implementation of a search algorithm, that returns the location of
  an item in an unsorted list.  If the list may contain duplicates,
  and a developer has not accounted for this possibility, the
  algorithm may return different locations of an item for the same list,
  depending on the scheduling of threads.  Arguably, this can be
  subsumed under the first class, with the behavior of the
  processor/scheduler as the ``enviromental'' factor; relying on
  scheduling is, in a sense, equivalent to calling {\tt random} or
  {\tt time}.
\end{itemize}

However, simply running a test twice in the same process (possibly with a delay between steps) does
not suffice in all cases.

\subsubsection{Process-Based Nondeterminism}

\label{sec:pnondet}

Some sources of nondeterminism unfortunately require executing a test in a new
process environment, because the source is inherently tied to the
process in which code runs; simply re-executing a test in the same
process will not reveal the problem.

Address Space Layout Randomization (ASLR) \cite{ASLR}  is probably the most
important source of nondeterminism that arises (only) from change in
process.  ASLR scrambles the layout of memory of the process in which
an executable runs in order to make it harder to exploit memory-safety
vulnerabilities in code.  As a side effect, it
means that a test that, in some circumstances, causes a crash, may at
other times not fail at all.  Even if no inputs produce a crash,
however, memory errors may produce variation in values that are
overwritten or arise from uninitialized memory, and thus be detectable
as nondeterminism.

Another example of process-based nondeterminism was discovered by numerous Python developers when Python
version 3.3 introduced automatic random salting of hashes on a
per-process basis (a new salt chosen for each Python process),
in order to mitigate hash-based denial of service attacks
\cite{denial}.  Until version 3.6 this not only resulted in changes in
exact hash values, but in the order of iteration on
dictionaries\footnote{In 3.6, the default dictionary
  implementation became an ordered dictionary with consistent
  iteration, though actual hashes remained nondeterministic.}.  While
relatively few programs rely on exact hash values, many implicitly
relied on them in that they only functioned correctly with a
predictable order for dictionary iteration.  Testing
Python code for nondeterminism based on this hash seed requires
running in a new process:  Python does not allow changing the salt, since changing hash values on-the-fly would
break all existing dictionaries and other structures relying on
hashing.

Other, less common, effects tied to process also exist.  A few tests
may somehow depend on their actual PID (Process ID) or at least the
parent process' ID.  Process change increases the likelihood that on a multi-core
system a test will execute on a different CPU than in the first
execution, which can have many subtle effects, mostly related to
changed timing.


\begin{table*}
\centering
{\scriptsize 

\caption{TSTL Method Calls for Nondeterminism Detection}
\label{tab:methods}
\begin{tabular}{l|l}
%\hline
Name & Purpose \\
\hline
\hline
{\tt nondeterministic} & returns True if test exhibits final state nondeterminism in $k$
  tries with delay $d$, optionally only over pools in $\mathcal{P}$\\
\hline
{\tt stepNondeterministic} & as above, except checks all visible values
                       (compares state at every step)\\
\hline
{\tt processNondeterministic} & as above, except runs test provided in
                          subprocess and compares non-opaque output
                          values \\
\hline
{\tt P} & returns probability that a predicate holds for a test (with
          optional number of sample to use in estimate) \\
\hline
{\tt forceP} & wraps a predicate for test reduction, checking that it
               has desired probability of holding, using
         sample size and replications parameters \\
\hline
\hline

%\hline
\end{tabular}
}
\end{table*}

\begin{table*}
\centering
{\scriptsize
\caption{TSTL Command Line Options for
  Nondeterminism Detection}
\label{tab:options}
\begin{tabular}{l|l|l}
%\hline
Name &  Tool(s) & Purpose \\
\hline
\hline
{\tt --checkDeterminism} & {\tt rt}, {\tt reduce} & Checks tests for deterministic
                                  behavior (visible value in rt, final
  value in reduce)\\
\hline
{\tt --checkProcessDeterminism} & {\tt rt}, {\tt reduce} & Checks determinism using more 
                                         expensive process-changed 
                                         execution \\
\hline
{\tt --determinismTries INT} & {\tt rt}, {\tt reduce} & Number of times to repeat test 
                                  checking for nondeterministic
                                  behavior \\
\hline 
{\tt --determinismDelay FLOAT} & {\tt rt}, {\tt reduce} & Delay between steps when 
                                  re-executing to check for
                                              determinism \\
\hline 
{\tt --checkFailureDeterminism} & tstl compiler & Produce a harness
                                                  that checks
                                                  determinism on all
                                                  failing actions \\
\hline 
{\tt --probability P} & {\tt reduce} & force property with respect to which 
                               test is being reduce to hold with 
                                 probability $P$ \\
\hline 
{\tt --samples S} & {\tt reduce} & use $S$ samples when checking predicate
                             probability\\
\hline 
{\tt --replications R} & {\tt reduce} & use $R$ replications when checking predicate probability\\
\hline
\hline

%\hline
\end{tabular}
}
\end{table*}

\subsection{A Brief Introduction to TSTL}

We implemented our approach in the TSTL \cite{NFM15,ISSTA15,tstlsttt}
system, an open-source
language and tool for property-based testing
\cite{Hypothesis,ClaessenH00} of Python code.   TSTL, including full
support for our nondeterminism detection methods, is available at
\url{https://github.com/agroce/tstl}, and the {\tt examples} directory
contains the source code needed to duplicate all of our experiments;
TSTL can also be installed using Python's {\tt pip} tool, by typing
{\tt pip install tstl}; you will need to clone the github repository
to have access to the case studies used in the paper, however.

TSTL has
been used to detect (and usually fix) errors in a number of widely
used Python libraries, the Python implementation itself, the Solidity
compiler and a Solidity static analysis tool, and Mac OS \cite{tstl}.

\subsection{Implementing Horizontal Nondeterminism Detection}

Because TSTL supports differential testing \cite{tstlsttt}, horizontal
nondeterminism detection can technically be implemented simply by
declaring a system to be its own reference, using TSTL's notation for
differential testing.  However, such an approach requires considerable effort
on the part of the user to express which values are checked for
equivalence, and does not (without a great deal of effort) support injecting timing differences,
or re-executing in a new process, in checking for nondeterminism.

We therefore instead made horizontal nondeterminism detection a first-class
property in TSTL, using TSTL's existing notation for marking some
types of values as \emph{opaque} (not usefully compared for equality),
used in automatic abstraction-based testing.  In TSTL, a test is a
sequence of \emph{actions}; parameter values to method and function
calls of the Software Under Test (SUT) are stored in finite-sized variable \emph{pools}.  A
TSTL test is (essentially) a sequence of assignments to pool values,
and method/function calls using pool values (including method calls on
pool objects).  This formalism is a widely used and efficient way to
represent unit tests \cite{Pacheco,AndrewsTR}.  In TSTL, \emph{visible
value determinism} is based on comparing the values of \emph{all pool
variables} after each test action (pool values other than the one
assigned to must also be compared, since a common source of
nondeterminism is when a call results in a change to a value passed as
a parameter, changes a shared reference held by two values).
TSTL omits comparison only of pools marked as {\tt OPAQUE} in the
TSTL language test harness (and values that do not support equality
checking).  \emph{Final State Determinism} simply performs the same
comparison, but only on the final values of all pool variables (or a
set of designated pools) after a test has
finished executing.

 Tables \ref{tab:methods}
and \ref{tab:options} show, respectively, the primary additions to the
TSTL Python API and the TSTL command line tools.  The TSTL test
generation tool is called {\tt rt}, since the default mode is a fast
pure Random Tester, and {\tt reduce} is TSTL's tool for
delta-debugging and test
normalization (more aggressive reduction that also tends to ease
debugging and test triage by making tests failing due to the same
fault more similar \cite{onetest}).  With these additions,
developers of TSTL-based testing tools, or developers using TSTL tools
to test code can easily add nondeterminism to the set of properties
checked.  It is even possible to write a TSTL harness that
automatically checks itself for nondeterminism, even if the random
tester is not run with {\tt --checkDeterminism}, simply by adding:

{\scriptsize
\begin{code}
property: not self.nondeterministic(self.test())
\end{code}
}

\noindent to the harness; in fact, by removing the {\tt not}, a test harness can
even specify that behavior should not be deterministic, a property of
possible use in some security-related libraries.

\subsection{Implementing Vertical Nondeterminism Detection}

First-class vertical nondeterminism checks are currently
limited to failures:  {\tt --checkFailureDeterminism} causes the TSTL
compiler to emit a harness where every action that causes an expected
exception to be raised is repeated to ensure the same exception is
still raised.  Users can also express that any action should exhibit
vertical determinism by wrapping the action in a function that checks
for failure and calls the code again.

\subsection{Tools for Nondeterministic Delta-Debugging}

Unlike the other additions, explicitly designed for nondeterminism
detection, the {\tt P} query, {\tt forceP} wrapper, and {\tt --probability}, {\tt
  --samples}, and {\tt --replications} reducer options are general
tools for use with nondeterministic predicates in delta-debugging.  Of
course, nondeterminism and flakiness are the most common probabilistic
test properties we are aware of, but in theory these tools can be used
for other probabilistic debugging problems (e.g., if the property of
interest is that a value is above a certain threshold in 90\% of test
executions, these TSTL additions can be used to find a small test where this property
fails to hold).  In addition to the basic probabilistic reduction
issues discussed above, actual implementation of test reduction for
nondeterminism introduces some subtle issues.  By default, for
example, TSTL automatically removed non-enabled actions from tests
during reduction, because these are never actually executed.  However,
in timing based nondeterminism, such steps may be essential to
causing the nondeterminism to appear (we had to turn off this feature
for the {\tt redis-py} example discussed below).

\subsection{Nondeterminism and AFL Stability}

Because TSTL has a first-class interface to AFL \cite{aflfuzz}, we can even use
AFL's sophisticated heuristics to perform very thorough, week-long
checks for nondeterminism, using strategies built to find subtle
security vulnerabilities in C programs.  Equally importantly, since
there is substantial overhead in nondeterminism detection, AFL can be
used to predict whether a program has potential nondeterminism; if the
AFL \emph{stability} statistic (see the AFL documentation for details
\cite{aflfuzz}) is lower than 100\%, it may indicate a problem.
Because of the idiosyncrasies of AFL instrumentation and process
behavior, this is not always a reliable guide, but it is a very low
cost indicator produced as a by-product of fuzzing.

\section{A Simple Example}