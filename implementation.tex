\section{Implementation}


\subsection{Sources of Nondeterminism in Practice}

In order to implement a practical nondeterminism-detection tool, it is
important to consider the sources of nondeterminism.  Most of these
sources can, in principle, be detected simply by re-running a test
twice and comparing results, either at all visible values or at the
final state.  For example, if some library call uses a random number
generator (perhaps in a stochastic algorithm), the value should be
different on a second run.  Timing dependencies may change on a second
execution, especially if a delay is added between steps of the test (a
more principled version of the unfortunate tradition of testing
concurrent code by adding ad-hoc {\tt sleep} statements).   Most
nondeterministic behavior in tests is probably attributable to a such
effects that can vary with a repeated execution, in the same
(approximate) enviroment, of the same test.  The two broad classes of
nondeterminism that can usually be detected (with some probability) by
simply executing a test twice in succession are:

\begin{itemize}
\item {\bf External Environment:} This class of nondeterminism arises
  when a test's behavior depends on factors outside the program under
  test, which are subject to uncontrolled variance.  Calling {\tt
    random()} in Python is an obvious example; the call is almost
  guaranteed to return a different value each time it is called in a
  test, unless we explicitly re-seed the random number generator at
  the beginning of each test.  Similarly, calls to {\tt time}
  functions  will usually return different values on different runs,
  and even if only elapsed time, rather than absolute, is used to
  influence behavior, subtle variances in timing are to be expected
  due to system load, I/O, and other external factors.  Finally, in
  distributed system testing, the environment is often remote servers
  (or clients) that may be unavailable, slower than expected, or
  subject to faulty behavior for reasons not under the control of the test.
\item {\bf Concurrency:} Perhaps the second most common cause for
  nondeterminism in tests is when an algorithm is implemented using
  concurrency, in the (incorrect) beleif that the result of the
  algorithm remains deterministic.  E.g., consider a multi-threaded
  implementation of a search algorithm, that returns the location of
  an item in an unsorted list.  If the list may contain duplicates,
  and a developer has not accounted for this possibility, the
  algorithm may return different locations of an item for the same list,
  depending on the scheduling of threads.  Arguably, this can be
  subsumed under the first class, with the behavior of the
  processor/scheduler as the ``enviromental'' factor; relying on
  scheduling is, in a sense, equivalent to calling {\tt random} or
  {\tt time}.
\end{itemize}

However, simply running a test twice in the same process (possibly with a delay between steps) does
not suffice in all cases.

\subsubsection{Process-Based Nondeterminism}

\label{sec:pnondet}

Some sources of nondeterminism unfortunately require executing a test in a new
process environment, because the source is inherently tied to the
process in which code runs; simply re-executing a test in the same
process will not reveal the problem.

Address Space Layout Randomization (ASLR) \cite{ASLR}  is probably the most
important source of nondeterminism that arises (only) from change in
process.  ASLR scrambles the layout of memory of the process in which
an executable runs in order to make it harder to exploit memory-safety
vulnerabilities in code.  As a side effect, it
means that a test that, in some circumstances, causes a crash, may at
other times not fail at all.  Even if no inputs produce a crash,
however, memory errors may produce variation in values that are
overwritten or arise from uninitialized memory, and thus be detectable
as nondeterminism.

Another example of process-based nondeterminism was discovered by numerous Python developers when Python
version 3.3 introduced automatic random salting of hashes on a
per-process basis (a new salt chosen for each Python process),
in order to mitigate hash-based denial of service attacks
\cite{denial}.  Until version 3.6 this not only resulted in changes in
exact hash values, but in the order of iteration on
dictionaries\footnote{In 3.6, the default dictionary
  implementation became an ordered dictionary with consistent
  iteration, though actual hashes remained nondeterministic.}.  While
relatively few programs rely on exact hash values, many implicitly
relied on them in that they only functioned correctly with a
predictable order for dictionary iteration.  Testing
Python code for nondeterminism based on this hash seed requires
running in a new process:  Python does not allow changing the salt, since changing hash values on-the-fly would
break all existing dictionaries and other structures relying on
hashing.

Other, less common, effects tied to process also exist.  A few tests
may somehow depend on their actual PID (Process ID) or at least the
parent process' ID.  Process change increases the likelihood that on a multi-core
system a test will execute on a different CPU than in the first
execution, which can have many subtle effects, mostly related to
changed timing.


\begin{table*}
\centering
{\scriptsize 

\caption{TSTL Method Calls for Nondeterminism Detection}
\label{tab:methods}
\begin{tabular}{l|l}
%\hline
Name & Purpose \\
\hline
\hline
{\tt nondeterministic} & returns True if test exhibits final state nondeterminism in $k$
  tries with delay $d$, optionally only over pools in $\mathcal{P}$\\
\hline
{\tt stepNondeterministic} & as above, except checks all visible values
                       (compares state at every step)\\
\hline
{\tt processNondeterministic} & as above, except runs test provided in
                          subprocess and compares non-opaque output
                          values \\
\hline
{\tt P} & returns probability that a predicate holds for a test (with
          optional number of sample to use in estimate) \\
\hline
{\tt forceP} & wraps a predicate for test reduction, checking that it
               has desired probability of holding, using
         sample size and replications parameters \\
\hline
\hline

%\hline
\end{tabular}
}
\end{table*}

\begin{table*}
\centering
{\scriptsize
\caption{TSTL Command Line Options for
  Nondeterminism Detection}
\label{tab:options}
\begin{tabular}{l|l|l}
%\hline
Name &  Tool(s) & Purpose \\
\hline
\hline
{\tt --checkDeterminism} & {\tt rt}, {\tt reduce} & Checks tests for deterministic
                                  behavior (visible value in rt, final
  value in reduce)\\
\hline
{\tt --checkProcessDeterminism} & {\tt rt}, {\tt reduce} & Checks determinism using more 
                                         expensive process-changed 
                                         execution \\
\hline
{\tt --determinismTries INT} & {\tt rt}, {\tt reduce} & Number of times to repeat test 
                                  checking for nondeterministic
                                  behavior \\
\hline 
{\tt --determinismDelay FLOAT} & {\tt rt}, {\tt reduce} & Delay between steps when 
                                  re-executing to check for
                                              determinism \\
\hline 
{\tt --checkFailureDeterminism} & tstl compiler & Produce a harness
                                                  that checks
                                                  determinism on all
                                                  failing actions \\
\hline 
{\tt --probability P} & {\tt reduce} & force property with respect to which 
                               test is being reduce to hold with 
                                 probability $P$ \\
\hline 
{\tt --samples S} & {\tt reduce} & use $S$ samples when checking predicate
                             probability\\
\hline 
{\tt --replications R} & {\tt reduce} & use $R$ replications when checking predicate probability\\
\hline
\hline

%\hline
\end{tabular}
}
\end{table*}

\subsection{A Brief Introduction to TSTL}

We implemented our approach in the TSTL \cite{NFM15,ISSTA15,tstlsttt}
system, an open-source
language and tool for property-based testing
\cite{Hypothesis,ClaessenH00} of Python code.   TSTL, including full
support for our nondeterminism detection methods, is available at
\url{https://github.com/agroce/tstl}, and the {\tt examples} directory
contains the source code needed to duplicate all of our
experiments\footnote{In some cases, you will also need to install an
  appropriate version of the tested software, by doing, e.g., {\tt pip
    install pyfakefs}.};
TSTL can also be installed using Python's {\tt pip} tool, by typing
{\tt pip install tstl}; you will need to clone the github repository
to have access to the case studies used in the paper, however.

TSTL has
been used to detect (and usually fix) errors in a number of widely
used Python libraries, the Python implementation itself, the Solidity
compiler and a Solidity static analysis tool, and Mac OS \cite{tstl}.

\begin{figure}
\begin{code}
pool: <int> 5
pool: <l> 5
pool: <s> 5
\vspace{0.1in}
<int> := <[1..20]>
<l> := []
<l> := list(<s>)
<s> := set()
<s> := set(<l>)
\vspace{0.1in}
<l>.append(<int>)
\{ValueError\} <l>.remove(<int>)
<s>.add(<int>)
\{KeyError\} <s>.remove(<int>)
\vspace{0.1in}
property: len(<l>) >= len(set(<l,1>))
property: len(<s>) == len(list(<s,1>))
\end{code}
\caption{Simple TSTL harness testing lists and sets}
\label{fig:simple}
\end{figure}

In TSTL, a test consists of a sequence of
\emph{actions}, where actions execute arbitrary Python code, but are
expected to work by modifying the \emph{state} of a test, which is
stored in a fixed set of \emph{pools} containing Python objects.  The
TSTL actions defined by a test harness correspond to the set $A$ of
actions in Section \ref{sec:formal}, and transitions between states of
result from carrying out the Python code in an action.  Figure
\ref{fig:simple} shows a very simple TSTL harness that tests Python's
built-in {\tt set} and {\tt list} types.  The harness defines the
(visible) state of a test as consisting of three named pools of Python
objects, {\tt int}, {\tt l}, and {\tt s}.  These pools have no
inherent associated type, though a type can be annotated as an
additional constraint that TSTL will check holds during test
execution.  Initially all the pools hold the special Python value {\tt
  None} indicating they have not been initialized.  In terms of our formalism,
$I = \{${\tt [<int> = [None, None, None, None, None]}, {\tt <l> =
  [None, None, None, None, None]}, {\tt <s> = [None, None, None, None,
  None]]}$\}$.  Actions with the {\tt :=} assignment operator
initialize pools.  For example, the line of code {\tt l := []} defines a set of 5
actions, each of which initializes a different member of the list pool
{\tt l} to an empty list --- {\tt l0 = []} ... {\tt l4 = []}.  Actions
without a {\tt :=} require that pools used in them already be
initialized.  TSTL provides constructs like {\tt <[1..20]>} for
choosing values in a range for use in a test, and allows a user to
specify which exceptions an action may raise without causing the test
to fail (as in {\tt \{KeyError\} <s>.remove(<int>)}).

Compiling this harness using
the TSTL compiler produces a Python module that allows test
generation.  If we use the random testing tool provided by TSTL, {\tt
  tstl\_rt}, it will randomly generate valid sequences of actions, and
report no problems.  If we change the first property to read {\tt
  property: len(<l>) == len(set(<l,1>))}, however, TSTL will report a
failing test, shown in Figure \ref{fig:setlistfail}.

\begin{figure}
{\scriptsize
\begin{code}
int0 = 1                                                                 \# STEP 0
ACTION: int0 = 1 
int0 = None : <type 'NoneType'>
=> int0 = 1 : <type 'int'>
==================================================
l0 = []                                                                  \# STEP 1
ACTION: l0 = [] 
l0 = None : <type 'NoneType'>
=> l0 = [] : <type 'list'>
==================================================
l0.append(int0)                                                          \# STEP 2
ACTION: l0.append(int0) 
int0 = 1 : <type 'int'>
l0 = [] : <type 'list'>
=> l0 = [1] : <type 'list'>
==================================================
l0.append(int0)                                                          \# STEP 3
ACTION: l0.append(int0) 
int0 = 1 : <type 'int'>
l0 = [1] : <type 'list'>
=> l0 = [1, 1] : <type 'list'>
==================================================
ERROR: (<type 'exceptions.AssertionError'>,
  AssertionError(), <traceback object at 0x1032b83f8>)
TRACEBACK:
  File "/Users/adg326/scratch/pyexample/sut.py",
  line 13695, in check
    assert len(self.p\_l[0]) == len(set(self.p\_l[0]))
\end{code}
}
\caption{A failing test (showing that converting a list to a set may
  not preserve the size of the object).}
\label{fig:setlistfail}
\end{figure}

\subsection{Implementing Horizontal Nondeterminism Detection}

Because TSTL supports differential testing \cite{tstlsttt}, horizontal
nondeterminism detection can technically be implemented simply by
declaring a system to be its own reference, using TSTL's notation for
differential testing.  However, such an approach requires considerable effort
on the part of the user to express which values are checked for
equivalence, and does not (without a great deal of effort) support injecting timing differences,
or re-executing in a new process, in checking for nondeterminism.

We therefore instead made horizontal nondeterminism detection a first-class
property in TSTL, using TSTL's existing notation for marking some
types of values as \emph{opaque} (not usefully compared for equality),
used in automatic abstraction-based testing.  Recall that in TSTL, a
test is (essentially) a sequence of assignments to pool values,
and method/function calls using pool values (including method calls on
pool objects).  This formalism is a widely used and efficient way to
represent unit tests \cite{Pacheco,AndrewsTR}.  In TSTL, \emph{visible
value determinism} is based on comparing the values of \emph{all pool
variables} after each test action (pool values other than the one
assigned to must also be compared, since a common source of
nondeterminism is when a call results in a change to a value passed as
a parameter, changes a shared reference held by two values).  By
default $V$ as defined in Section \ref{sec:formal} is just the set of all possible pool values, and $v$
simply extracts the pool values from $S$.
TSTL allows restriction of which state is visible by removing pools marked as {\tt OPAQUE} in the
TSTL language test harness (and values that do not support equality
checking) from $v(S)$.  \emph{Final State Determinism} simply performs the same
comparison, but only on the final values of all pool variables (or a
set of designated pools) after a test has
finished executing.

 Tables \ref{tab:methods}
and \ref{tab:options} show, respectively, the primary additions to the
TSTL Python API and the TSTL command line tools.  The TSTL test
generation tool is called {\tt rt}, since the default mode is a fast
pure Random Tester, and {\tt reduce} is TSTL's tool for
delta-debugging and test
normalization (more aggressive reduction that also tends to ease
debugging and test triage by making tests failing due to the same
fault more similar \cite{onetest}).  With these additions,
developers of TSTL-based testing tools, or developers using TSTL tools
to test code can easily add nondeterminism to the set of properties
checked.  It is even possible to write a TSTL harness that
automatically checks itself for nondeterminism, even if the random
tester is not run with {\tt --checkDeterminism}, simply by adding:

{\scriptsize
\begin{code}
property: not self.nondeterministic(self.test())
\end{code}
}

\noindent to the harness; in fact, by removing the {\tt not}, a test harness can
even specify that behavior should not be deterministic, a property of
possible use in some security-related libraries.

\subsection{Implementing Vertical Nondeterminism Detection}

First-class vertical nondeterminism checks are currently
limited to failures:  {\tt --checkFailureDeterminism} causes the TSTL
compiler to emit a harness where every action that causes an expected
exception to be raised is repeated to ensure the same exception is
still raised.  Users can also express that any action should exhibit
vertical determinism by wrapping the action in a function that checks
for failure and calls the code again.

\subsection{Tools for Nondeterministic Delta-Debugging}

Unlike the other additions, explicitly designed for nondeterminism
detection, the {\tt P} query, {\tt forceP} wrapper, and {\tt --probability}, {\tt
  --samples}, and {\tt --replications} reducer options are general
tools for use with nondeterministic predicates in delta-debugging.  Of
course, nondeterminism and flakiness are the most common probabilistic
test properties we are aware of, but in theory these tools can be used
for other probabilistic debugging problems (e.g., if the property of
interest is that a value is above a certain threshold in 90\% of test
executions, these TSTL additions can be used to find a small test where this property
fails to hold).  In addition to the basic probabilistic reduction
issues discussed above, actual implementation of test reduction for
nondeterminism introduces some subtle issues.  By default, for
example, TSTL automatically removed non-enabled actions from tests
during reduction, because these are never actually executed.  However,
in timing based nondeterminism, such steps may be essential to
causing the nondeterminism to appear (we had to turn off this feature
for the {\tt redis-py} example discussed below).

\subsection{Nondeterminism and AFL Stability}

Because TSTL has a first-class interface to AFL \cite{aflfuzz}, we can even use
AFL's sophisticated heuristics to perform very thorough, week-long
checks for nondeterminism, using strategies built to find subtle
security vulnerabilities in C programs.  Equally importantly, since
there is substantial overhead in nondeterminism detection, AFL can be
used to predict whether a program has potential nondeterminism; if the
AFL \emph{stability} statistic (see the AFL documentation for details
\cite{aflfuzz}) is lower than 100\%, it may indicate a problem.
Because of the idiosyncrasies of AFL instrumentation and process
behavior, this is not always a reliable guide, but it is a very low
cost indicator produced as a by-product of fuzzing.

\section{A Simple Example}