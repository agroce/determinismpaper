\section{Introduction}

For ourselves, we might prefer to think (and act as if) we had free
will; however, we generally prefer our software systems to be as
constrained in their actions as possible:  in other words, we wish
them to be largely deterministic, from our perspective.

Determinism is particularly important for testing and debugging, where
being able to exactly reproduce system behavior is essential to
productivity.  Developers using a test exhibiting nondeterminism
to debug a system
face a serious challenge.
Regression testing
effectiveness can be significantly reduced if tests cover code or,
worse yet, fail only
intermittently and unpredictably.  Such behavior is
unfortunately all too common:  Gao et
al. \cite{Gao:2015:MSU:2818754.2818764} observed coverage differences
of up to 184 LOC for the same test, and false positive rates
as high as 96\%.  Nondeterminism is problematic for
developers, who want to  assume that library code behaves in a  predictable fashion; nondeterminism is
vexing in debugging; nondeterminism is \emph{often disastrous} for large-scale 
automated testing.


%\subsection{What is Determinism?}

A system is deterministic if, given the system's complete state at a point in
time, it is possible, in principle, to predict its future behavior
perfectly.  In the real world,
prediction may be possible but  impractical, or some details neither
predictable nor interesting.  We write
complex software systems because we cannot predict their
behavior, and we usually want to abstract away from machine-level details.  As a consequence, rather than
defining determinism in terms of prediction, we usually 
say that a system is deterministic if, \emph{given a certain limited
  higher-level abstraction of state
and inputs, observable behavior is repeatable}.  %This abstraction,
%for most software, is not expected to include many elements outside of
%the software system itself.

\begin{comment}
Because the
programmer's abstraction of the system ignores a large number of
details, very few tests are ``deterministic'' in the sense
that they eliminate all changes between executions.  Running the same test
twice almost always results in differences, given a low enough level of
abstraction, since the system load,
cache contents, branch predictor history, etc. are almost never
controlled for; however, what matters is when
some kind of nondeterminism unexpectedly impacts computed values at a
higher level of abstraction.
\end{comment}

\begin{comment} in general, if the operating system
itself is not buggy, low-level nondeterminism, by design, is invisible
except in fine-grained performance testing or real-time systems.
Unexpected nondeterminism usually arises when there is an element of higher-level
state or input that \emph{is} critical to the produced behavior, but
the programmer has not anticipated.  E.g.,  when it is believed that the
behavior of a thread scheduler will not matter, but a race condition
in the code means that it does matter, or when the order of items in
an iterator on a hash table is important, and the hash values used are
randomly salted.
\end{comment}

%\subsection{The High Cost of Unexpected Nondeterminism}

Unexpected nondeterminism is, unfortunately, usually only discovered
in a context that makes it very hard to debug.   The most common such
contexts are occasional rare failures of a system in deployment, and
regression tests that do not behave reliably (known as flaky tests).
Furthermore, unexpected nondeterminism makes it difficult to use
automated test generation to produce effective regression tests for a
system.  While developers may know how to produce reliably
deterministic unit tests, automated test generation usually does not
have sufficient information to do this.
%without considerable human effort.

%\subsection{Nondeterminism and Flaky Tests}

Complex
modern software systems usually include a large set of
\emph{regression tests}.  A regression test suite is a
set of tests that can be run every time code
is modified, to ensure that the modification has not broken the system.  \emph{Flaky tests} \cite{miccoflaky} are regression
tests that fail in an intermittent, unreliable fashion.  The essence of a flaky
test is that, for the same snapshot of test code and code under test, it sometimes fails and sometimes passes: the pass/fail result (disposition) of the
test is not a deterministic property of the test code, code under
test, and testing environment.  This produces three serious
problems: first, a flaky test often wastes developer time and delays
software changes by forcing the investigation of \emph{correct} code-under-test.  Second, failures in flaky tests are often ignored, and therefore serious software faults missed.  Finally, to mitigate the problem, flaky tests are often
run multiple times, wasting computing resources and
delaying code changes.  Flaky tests are, for us, simply a special case the 
\emph{horizontal} nondeterminism 
described below.  Our focus is on detecting sources of 
flaky-ness, \emph{before} they propagate.
\begin{comment}
  As an analogy, we note that  a canary in a coal mine is of
little use if canaries frequently become ill for reasons unrelated to
the presence of toxic gases.  Mining may stop for no good reason, or
miners may learn to ignore the canary, leading to tragedy; a third,
more ``practical'' option is that miners may carry so many redundant
canaries into the coal mine that canary-care itself becomes a serious burden.
\end{comment}


%unlike all other tools and
%approaches we are aware of, is on detecting the \emph{sources} of
%flaky-ness \emph{before} they propagate to cause actual test failure.

%\subsection{Contributions}


{\bf Contributions:} This paper proposes (1) a number of formal definitions of types of
nondeterminism (\emph{horizontal} and \emph{vertical}) and (2) an implementation, based on these definitions, for detecting and debugging
nondeterminism in property-based testing.  The implementation is based
on an approach where (3) horizontal determinism is considered as a
kind of \emph{reflexive differential testing} (4) vertical
determinism is specialized to the common case of \emph{failure
  determinism}, and (5)  in both cases the formalism is made practical
by using the \emph{value pool} model of unit tests.
We also introduce necessary modifications to the widely used delta-debugging algorithm in order to better handle
nondeterminism as a test property.  Evaluating these approaches with
realistic Python libraries, we show large improvements to both fault
detection and test reduction.

\begin{comment}
Previous practical approaches to
nondeterminism/flaky test detection and debugging rely either on 1) identifying
potential sources of nondeterminism such as test inter-dependence
\cite{LamZE2015} or certain code smells \cite{palomba2017does}  or 2)
use a heuristic approach to mark failures as likely flaky, such as that taken by DeFlaker
\cite{bell2018d}.  In the real world, people usually detect flaky
tests by the most obvious heuristic of all:  they observe a test both
fail and pass for the same code version.  In a sense, even DeFlaker
relies on this method, but avoids having to observe a flaky test actually
passing, by inferring that if a test was previously passing, and
executes no changed code, its failure after a code change \emph{must}
be due to flakiness/nondeterminism.  Our approach to the problem is
fundamentally different, and essentially orthogonal; we concentrate on the detection and avoidance of
nondeterminism/flakiness using \emph{automated test generation},
and make it possible for a developer or test engineer to detect when
important values generated in automated testing differ without a
change in the test itself or the code under test.  While some of these
changes may be harmless, and unable to propagate to cause test
failures, they are potential sources of flaky behavior.  To our
knowledge, this is the first approach to the problem that
allows detection of (potential sources of) flakiness, based on real divergence in
behavior, without having to observe a test actually fail due to
nondeterminism.  we additionally introduce the first variation of
delta-debugging that properly handles probabilistic faults and
reduction properties.
\end{comment}

\begin{comment}
Our methods enable automated production of
nondeterminism-free regression tests even for libraries with some
sources of nondeterminism, documentation of sources of
nondeterminism that might produce problems for manually-constructed
tests, and, most importantly, detection and debugging of 
unexpected nondeterminism bugs.  We use a set
of case studies. including real-world, widely-used, Python libraries, to demonstrate the
utility of our ideas.  Our novel notion of failure determinism directly
produces a more than 5\% improvement in mutation score for an already highly
effective automated test generation tool for a file system, with no
additional specification or test design burden.
\end{comment}