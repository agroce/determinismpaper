\section{Related Work}

At a very broad level, the topic of uncertainty (and thus nondeterminism) in software engineering has been addressed by Garlan \cite{GarlanUncertain}, Elbaum and Rosenblum \cite{Unknowns,lu2015roundtable}, and Ziv and Richardson \cite{UncertaintyPrinciple}.  There is a general agreement that as systems become more complex, more distributed, and more \emph{statistical}, these problems will only grow \cite{lu2015roundtable}.

Gao et al. generally considered the question of how to make tests repeatable \cite{Gao:2015:MSU:2818754.2818764}, in the context of system-wide user interaction tests.  Their work focused on systemic factors, such as execution platform and persistent configuration and database files, in contrast to our focus on identifying nondeterminism at the library level.  However, what their ``test harness factors'' include delays, which can be of importance to nondeterminism at the library level.
Shi et. al \cite{DetermImp} examined what might be seen as a related, though in a sense, opposite, problem:  they detect code that assumes a deterministic implementation of a non-deterministic specification.  E.g., they detect instances when  iteration through items of a set is explicitly not guaranteed to provide any particular order of items, but code depends on the order. Determinism is also sometimes used as a property in domain-specific testing, e.g. for shader compilers \cite{shader}.

The problem of test nondeterminism is closely related to the (extremely important in industrial settings) issue of flaky tests \cite{miccoflaky, luo2014empirical,palomba2017does,listfieldtestanalysis}.  How to handle flaky tests in practice (when they cannot be avoided) is a major issue in Google-scale continuous testing \cite{memon2017taming}, and, as Memon et al. describe, the problem of flaky tests influences the general design of Google test automation.
Previous work on flaky tests has either focused on test inter-dependence as a cause of flaky behavior \cite{LamZE2015}, or provided large-scale empirical examination of tests from one large open source project (Apache) \cite{luo2014empirical,palomba2017does}.  Palomba and Zaidman \cite{palomba2017does} investigated the relationship between test code smells and flaky tests.

Bell et al. proposed DeFlaker \cite{bell2018d}, which makes flaky tests much easier to detect by relying on the observation that if a test fails, and does not cover any changed code then (1) presumably it was a passing test in the past, or the tests would not be ``green'' before the change so (2) the test is likely flaky.  Comparing our approach with DeFlaker is difficult; DeFlaker applies only in the context of code \emph{changes} and is essentially a heuristic that identifies failures as due to nondeterminism.  Our approach is primarily focused on automatically providing a more extensive \emph{specification} in automated test generation, where a test ``failing'' is itself a sign of nondeterministic behavior.  iDFlakies \cite{idflakies} is a framework and dataset for flaky tests, but again focuses on whole-tests, and detecting flakiness by actually observing it.  Their work shows the order-dependence accounts for a little more than 50\% of flaky test causes, and they (like us) classify the rest as due to concurrency, timeouts,
network/IO, etc.---the kinds of cause that our approach focuses on identifying.
The present paper, rather than focusing on flaky tests as such, investigates methods for handling nondeterminism in property-based test generation \cite{ClaessenH00,Papadakis:2011:PIT:2034654.2034663}.  From our point of view, flaky behavior is simply a special case of nondeterminism, where the nondeterminism is sufficient to cause the test to have different pass/fail results.  Our approach is, in a sense (especially the delta-debugging modifications) in line with Harman and O'Hearn's proposal to simply accept that ``All Tests Are Flaky'' \cite{StartupstoScaleups}, and work with probabilistically failing tests.  To our
knowledge, this is the first approach to the problem that
allows detection of potential sources of flakiness, based on real divergence in
behavior, without having to observe a test actually fail.  We additionally introduce the first variation of
delta-debugging that properly handles probabilistic reduction criteria.

Other efforts \cite{ParallelDeterministic} have aimed to avoid nondeterminism in parallel implementations, by design, indicating the importance of avoiding nondeterministic behavior, even at the expense of adopting novel programming models, where the goal is not simply (as in, say, Rust) to avoid classic concurrency errors, but to enforce true determinism.

Vertical/failure determinism is a less-studied concept, and to our knowledge the formulation here, as a kind of determinism in a different ``direction,'' has not previously appeared; the kinds of errors that are exposed, however, are not new.  In particular, we believe that many such faults are related to the propagation of error conditions, which has been studied using static analysis \cite{FileProp}.

TSTL's approach to test generation, and our instantiation of the formal definitions of nondeterminism are based on a formulation of unit tests using \emph{pools} of values \cite{AndrewsTR}, which provides a practical solution to the problem of defining the visible state to be compared when checking for nondeterminism.

\begin{comment}
Finally, Cotroneo et al. \cite{CompBugs,FaultTriggers}, and Grottke and Trivedi \cite{GrottkeBugs} have followed on early work on understanding bugs and how they manifest, including transient \cite{Transient} software faults.  This work informs our attempt to identify sources of nondeterminism, and should provide other, context and project-specific, sources that could be introduced into our general framework.
\end{comment}