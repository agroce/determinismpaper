\section{Related Work}

At a very broad level, the topic of uncertainty (and thus nondeterminism) in software engineering has been addressed by Garlan \cite{GarlanUncertain}, Elbaum and Rosenblum \cite{Unknowns,lu2015roundtable}, and Ziv and Richardson \cite{UncertaintyPrinciple}.  There is a general agreement that as systems become more complex, more distributed, and more \emph{statistical}, these problems will only grow \cite{lu2015roundtable}.

Gao et al. generally considered the question of how to make tests repeatable \cite{Gao:2015:MSU:2818754.2818764}, in the context of system-wide user interaction tests.  Their work focused on systemic factors, such as execution platform and persistent configuration and database files, in contrast to our focus on identifying nondeterminism at the library level.  However, what they refer to as ``test harness factors'' includes delays, which can be of importance to nondeterminism at the library level when asynchronous behavior is involved.

Shi et. al \cite{DetermImp} examined what might be seen as a related, though in a sense the opposite, problem:  they detect code that assumes a deterministic implementation of a non-deterministic specification.  E.g., they detect instances when  iteration through items of a set is explicitly not guaranteed to provide any particular order of items, but code depends on the order produced by a given implementation.  This procedure, applied to Python code in a pre-3.3 environment, would have flagged many instances of the nondeterminism that arose on the introduction of salted hashing.  Determinism is also sometimes used as a property in domain-specific testing efforts, e.g. in testing shader compilers \cite{shader}.

The problem of test nondeterminism is closely related to the (extremely important in industrial settings) issue of flaky tests \cite{miccoflaky, luo2014empirical,palomba2017does,listfieldtestanalysis}.  How to handle flaky tests in practice (when they cannot be avoided) is a major issue in Google-scale continuous testing \cite{memon2017taming}, and, as Memon et al. describe, the problem of flaky tests influences the general design of Google test automation.
Previous work on flaky tests has either focused on test inter-dependence as a cause of flaky behavior \cite{LamZE2015}, or provided large-scale empirical examination of tests from one large open source project (Apache) \cite{luo2014empirical,palomba2017does}.  Palomba and Zaidman \cite{palomba2017does} investigated the relationship between test code smells and flaky tests.

Bell et al. proposed DeFlaker \cite{bell2018d}, which makes flaky tests much easier to detect by relying on the observation that if a test fails, and does not cover any changed code then (1) presumably it was a passing test in the past, or the tests would not be ``green'' before the latest code change so (2) the test is likely flaky.  Comparing our approach with that of DeFlaker is difficult; DeFlaker applies only in the context of code \emph{changes} and is essentially a heuristic that identifies certain test failures as due to nondeterminism.  Our approach is primarily focused on automatically providing a more extensive \emph{specification} in automated test generation, where a test ``failing'' is itself a sign of nondeterministic behavior, that could give rise to flaky behavior.

iDFlakies \cite{idflakies} is a framework and dataset for flaky tests, but again focuses on whole-tests, and detecting flakiness by actually observing it.  Their work shows the order-dependence accounts for a little more than 50\% of flaky test causes, and they (like us) classify the rest as due to ``concurrency, timeouts,
network/IO, etc.' --- the kinds of cause that our approach focuses on identifying.

The present paper, rather than focusing on flaky tests as such, therefore, investigates methods for handling nondeterminism in property-based test generation \cite{ClaessenH00,Papadakis:2011:PIT:2034654.2034663}.  Again, from our point of view, flaky behavior is simply a special case of nondeterminism, where the nondeterminism is sufficient to cause the test to have different pass/fail results at times.  Our approach is, in a sense (especially the delta-debugging modifications) in line with Harman and O'Hearn's proposal to simply accept that ``All Tests Are Flaky'' \cite{StartupstoScaleups}, and work with probabilistically failing tests.

Other efforts \cite{ParallelDeterministic} have aimed to avoid nondeterminism in parallel implementations, by design, indicating the importance of avoiding nondeterministic behavior, even at the expense of adopting novel programming models, where the goal is not simply (as in, say, Rust) to avoid classic concurrency errors, but to enforce genuinely deterministic behavior.

Vertical/failure determinism is a less-studied concept, and to our knowledge the formulation here, as a kind of determinism in a different ``direction,'' has not previously appeared; the kinds of errors that are exposed, however, are not new.  In particular, we believe that many faults discovered using failure determinism are related to the propagation of error conditions in code, which has been studied in, e.g., file systems, using static analysis \cite{FileProp}.

TSTL's approach to test generation, and our instantiation of the formal definitions of nondeterminism are based on a formulation of unit tests using \emph{pools} of values \cite{AndrewsTR}, which provides a practical solution to the problem of defining the visible state to be compared when checking for nondeterminism.

Finally, Cotroneo et al. \cite{CompBugs,FaultTriggers}, and Grottke and Trivedi \cite{GrottkeBugs} have followed on early work on understanding bugs and how they manifest, including transient \cite{Transient} software faults.  This work informs our attempt to identify sources of nondeterminism, and should provide other, context and project-specific, sources that could be introduced into our general framework.
