\section{Related Work}

At a broad level, the topic of uncertainty (and thus nondeterminism) in software engineering has been addressed by Garlan \cite{GarlanUncertain}, Elbaum and Rosenblum \cite{Unknowns,lu2015roundtable}, and Ziv and Richardson \cite{UncertaintyPrinciple}.  There is a general agreement that as systems become more complex, more distributed, and more \emph{statistical}, these problems will only grow \cite{lu2015roundtable}.
Gao et al. examined the question of how to make tests repeatable \cite{Gao:2015:MSU:2818754.2818764}, in the context of system-wide user interaction tests, focusing on systemic factors in contrast to our focus on  the library level.  
Shi et. al \cite{DetermImp} examined what might be seen as the opposite, problem:  they detect code that assumes a deterministic implementation of a non-deterministic specification.  %E.g., they detect instances when  iteration through items of a set is explicitly not guaranteed to provide any particular order of items, but code depends on the order. Determinism is also sometimes used as a property in domain-specific testing, e.g. for shader compilers \cite{shader}.

The problem of test nondeterminism is closely related to the issue of flaky tests \cite{miccoflaky, luo2014empirical,palomba2017does,listfieldtestanalysis}.  How to handle flaky tests in practice (when they cannot be avoided) is a major issue in Google-scale continuous testing \cite{memon2017taming}, and, as Memon et al. describe, the problem of flaky tests influences the general design of Google test automation.
Previous work on flaky tests has either focused on test inter-dependence \cite{LamZE2015}, or large-scale empirical examination \cite{luo2014empirical,palomba2017does}.  
Bell et al. proposed DeFlaker \cite{bell2018d}, which makes flaky tests much easier to detect by relying on the observation that if a test fails, and does not cover any changed code then the test is likely flaky.   iDFlakies \cite{idflakies} is a framework and dataset for flaky tests, but again focuses on whole-tests, and detecting flakiness by actually observing it.  Their work shows the order-dependence accounts for a little more than 50\% of flaky test causes, and they classify the rest as due to ``concurrency, timeouts,
network/IO, etc.''---the causes that our approach focuses on identifying.

We introduce the first variation of
delta-debugging that properly handles probabilistic reduction criteria, in line with Harman and O'Hearn's proposal to simply accept that ``All Tests Are Flaky'' \cite{StartupstoScaleups}, and work with probabilistically failing tests..
Vertical/failure determinism is also a less-studied concept, and to our knowledge our formulation is novel. The kinds of errors that are exposed, however, are not new.  In particular, we believe that many such faults are related to the propagation of error conditions, which has been studied using static analysis \cite{FileProp}.

%TSTL's approach to test generation, and our instantiation of the formal definitions of nondeterminism are based on a formulation of unit tests using \emph{pools} of values \cite{AndrewsTR}, which provides a practical solution to the problem of defining the visible state to be compared when checking for nondeterminism.

\begin{comment}
Finally, Cotroneo et al. \cite{CompBugs,FaultTriggers}, and Grottke and Trivedi \cite{GrottkeBugs} have followed on early work on understanding bugs and how they manifest, including transient \cite{Transient} software faults.  This work informs our attempt to identify sources of nondeterminism, and should provide other, context and project-specific, sources that could be introduced into our general framework.
\end{comment}