\section{Related Work}

Gao et al. generally considered the question of how to make tests repeatable \cite{Gao:2015:MSU:2818754.2818764}, in the context of system-wide user interaction tests.  Their work focused on systemic factors, such as execution platform and persistent configuration and database files, in contrast to our focus on identifying nondeterminism at the library level.  However, what they refer to as ``test harness factors'' includes delays, which can be of importance to nondeterminism at the library level when asynchronous behavior is involved.

Shi et. al \cite{DetermImp} examined what might be seen as a related, though in a sense the opposite, problem:  they detect code that assumes a deterministic implementation of a non-deterministic specification.  E.g., they detect instances when  iteration through items of a set is explicitly not guaranteed to provide any particular order of items, but code depends on the order produced by a given implementation.  This procedure, applied to Python code in a pre-3.3 environment, would have flagged many instances of the nondeterminism that arose on the introduction of salted hashing.  Determinism is also sometimes used as a property in domain-specific testing efforts, e.g. in testing shader compilers \cite{shader}.

The problem of test nondeterminism is closely related to the (extremely important in industrial settings) issue of flaky tests \cite{miccoflaky, luo2014empirical,palomba2017does,listfieldtestanalysis}.  How to handle flaky tests in practice (when they cannot be avoided) is a major issue in Google-scale continuous testing \cite{memon2017taming}, and, as Memon et al. describe, the problem of flaky tests influences the general design of Google test automation.
Previous work on flaky tests has either focused on test inter-dependence as a cause of flaky behavior \cite{LamZE2015}, or provided large-scale empirical examination of tests from one large open source project (Apache) \cite{luo2014empirical,palomba2017does}.  Palomba and Zaidman \cite{palomba2017does} investigated the relationship between test code smells and flaky tests.   The present paper, rather than focusing on flaky tests per se, investigates new tools for handling nondeterminism in property-based test generation.  From our point of view, flaky behavior is simply a special case of nondeterminism, where the nondeterminism is sufficient to cause the test to have different pass/fail results at times.

Other efforts \cite{ParallelDeterministic} have aimed to avoid nondeterminism in parallel implementations, by design, indicating the importance of avoiding nondeterministic behavior, even at the expense of adopting novel programming models, where the goal is not simply (as in, say, Rust) to avoid classic concurrency errors, but to enforce genuinely deterministic behavior.  

